{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4a0c83c01a84c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:57:15.363782Z",
     "start_time": "2024-09-24T19:57:12.177616Z"
    }
   },
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "import pyeCAP\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "from scipy import signal\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9506f677aa73d4d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:57:15.410567Z",
     "start_time": "2024-09-24T19:57:15.369782Z"
    }
   },
   "outputs": [],
   "source": [
    "'Functions'\n",
    "def pentaPLOT(param, ECAP_channel, params_to_tank_dict, metaDF, physDF, ECAP_data, EMG_data, TriCAP_data, phys_data, phys_channel, auto_scale_factor, remove_stim_artifact = False, offsets = None, save_fig = False):\n",
    "    \n",
    "    if isinstance(param, list):\n",
    "        raise Exception(\"Param must be a single parameter, not a list.\")\n",
    "    \n",
    "    tank_ID = params_to_tank_dict[param[0]]\n",
    "    HF_duration = metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['HF Duration (s)'].item()\n",
    "    \n",
    "    \"\"\"Grabs the pulse # of HF onset/offset using the standard deviation of the each pulse\"\"\"\n",
    "    HF_onset_pulse, HF_offset_pulse = find_HF(param, ECAP_data, ECAP_channel, metaDF, tank_ID, plot=False)\n",
    "    print(\"HF onset pulse #: \" + str(HF_onset_pulse))\n",
    "    print(\"HF offset pulse #: \" + str(HF_offset_pulse)) \n",
    "    \n",
    "    \"\"\"Pull time values, units converted to milliseconds.\"\"\"\n",
    "    plot_time = np.squeeze(ECAP_data.time(param) * 1e3) \n",
    "    \n",
    "    \"\"\"Pull signals for pre, during, and post-HF pulse.  Units converted to microvolts\"\"\"   \n",
    "    pre_HF_ECAP = np.squeeze(ECAP_data.mean(parameters=param, channels = ECAP_channel, bin=[0,HF_onset_pulse-1])[param]) * 1e6\n",
    "    HF_ECAP = np.squeeze(ECAP_data.mean(parameters=param, channels = ECAP_channel, bin=[HF_onset_pulse + 1,HF_offset_pulse - 1])[param]) * 1e6\n",
    "    post_HF_ECAP = np.squeeze(ECAP_data.mean(parameters=param, channels = ECAP_channel, bin=[HF_offset_pulse + 1, ECAP_data.stim.parameters.loc[param, 'pulse count']])[param]) * 1e6\n",
    "\n",
    "    pre_HF_TriCAP = np.squeeze(TriCAP_data.mean(parameters=param, channels = 0, bin=[0,HF_onset_pulse-1])[param]) * 1e6\n",
    "    HF_TriCAP = np.squeeze(TriCAP_data.mean(parameters=param, channels = 0, bin=[HF_onset_pulse + 1,HF_offset_pulse -1])[param]) * 1e6\n",
    "    post_HF_TriCAP = np.squeeze(TriCAP_data.mean(parameters=param, channels = 0, bin=[HF_offset_pulse + 1, TriCAP_data.stim.parameters.loc[param, 'pulse count']])[param]) * 1e6\n",
    "\n",
    "    pre_HF_EMG = np.squeeze(EMG_data.mean(parameters=param, bin=[0,HF_onset_pulse-1])[param]) * 1e6\n",
    "    HF_EMG = np.squeeze(EMG_data.mean(parameters=param, bin=[HF_onset_pulse + 1,HF_offset_pulse -1])[param]) * 1e6\n",
    "    post_HF_EMG = np.squeeze(EMG_data.mean(parameters=param, bin=[HF_offset_pulse + 1, EMG_data.stim.parameters.loc[param, 'pulse count']])[param]) * 1e6\n",
    "    \n",
    "    if offsets is not None:        \n",
    "        if not isinstance(offsets, list):\n",
    "            raise Exception(\"'offsets' must be a list of integer values of format [ECAP_offset, TriCAP_offset, EMG_offset] or 'None'.\")\n",
    "        else:                \n",
    "            ECAP_dy = np.std(pre_HF_ECAP) * offsets[0]\n",
    "            TriCAP_dy = np.std(pre_HF_TriCAP) * offsets[1]\n",
    "            EMG_dy = np.std(pre_HF_EMG, axis = 1) * offsets[2]\n",
    "            \n",
    "            pre_HF_ECAP = pre_HF_ECAP + ECAP_dy\n",
    "            post_HF_ECAP = post_HF_ECAP - ECAP_dy\n",
    "            \n",
    "            pre_HF_TriCAP = pre_HF_TriCAP + TriCAP_dy\n",
    "            post_HF_TriCAP = post_HF_TriCAP - TriCAP_dy\n",
    "            \n",
    "            pre_HF_EMG[0] = pre_HF_EMG[0] + EMG_dy[0]\n",
    "            post_HF_EMG[0] = post_HF_EMG[0] - EMG_dy[0]\n",
    "            pre_HF_EMG[1] = pre_HF_EMG[1] + EMG_dy[1]\n",
    "            post_HF_EMG[1] = post_HF_EMG[1] - EMG_dy[1]            \n",
    "            pre_HF_EMG[2] = pre_HF_EMG[2] + EMG_dy[2]\n",
    "            post_HF_EMG[2] = post_HF_EMG[2] - EMG_dy[2]\n",
    "    \n",
    "    fig,ax = plt.subplots(num =1, nrows = 3, ncols=2, figsize=(15,10), clear = True)\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    \"\"\" ECAP \"\"\"\n",
    "    ax[0].plot(plot_time, pre_HF_ECAP, color = 'b', label = 'Pre-HF')\n",
    "    ax[0].plot(plot_time, HF_ECAP, color = 'r', label = 'During HF')\n",
    "    ax[0].plot(plot_time, post_HF_ECAP, color = 'g', label = 'Post-HF')\n",
    "\n",
    "    if remove_stim_artifact:\n",
    "        ECAP_std_data = np.std(pre_HF_ECAP[60:])\n",
    "    else:\n",
    "        ECAP_std_data = np.std(pre_HF_ECAP)\n",
    "    \n",
    "    if offsets is not None:\n",
    "        ECAP_y_lim = [(-ECAP_std_data * auto_scale_factor) - ECAP_dy, (ECAP_std_data * auto_scale_factor) + ECAP_dy]    \n",
    "    else:        \n",
    "        ECAP_y_lim = [-ECAP_std_data * auto_scale_factor, ECAP_std_data * auto_scale_factor] \n",
    "    ax[0].set(ylim=(ECAP_y_lim[0],ECAP_y_lim[1]), xlim=(0,20), title='ECAP', ylabel = 'Voltage (uV)')\n",
    "    ax[0].legend(loc='upper right')\n",
    "\n",
    "    \"\"\"Tripolar ECAP\"\"\"\n",
    "    ax[1].plot(plot_time, pre_HF_TriCAP, color = 'b', label = 'Pre-HF')\n",
    "    ax[1].plot(plot_time, HF_TriCAP, color = 'r', label = 'During HF')\n",
    "    ax[1].plot(plot_time, post_HF_TriCAP, color = 'g', label = 'Post-HF')\n",
    "\n",
    "    if remove_stim_artifact:\n",
    "        triCAP_std_data = np.std(pre_HF_TriCAP[60:])\n",
    "    else:\n",
    "        triCAP_std_data = np.std(pre_HF_TriCAP)\n",
    "\n",
    "    if offsets is not None:\n",
    "        triCAP_y_lim = [(-triCAP_std_data * auto_scale_factor) - TriCAP_dy, (triCAP_std_data * auto_scale_factor) + TriCAP_dy]    \n",
    "    else:\n",
    "        triCAP_y_lim = [-triCAP_std_data * auto_scale_factor, triCAP_std_data * auto_scale_factor] \n",
    "    ax[1].set(ylim=(triCAP_y_lim[0],triCAP_y_lim[1]), xlim=(0,20), title='Tripolar ECAP', ylabel = 'Voltage (uV)')\n",
    "    ax[1].legend(loc='upper right')\n",
    "\n",
    "    #TODO: Add EMG channel ID's to metadata DF so that the function can autoplot the correct EMG\n",
    "    \"\"\" CA EMG \"\"\"\n",
    "    CA_chan = metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['CA EMG Channel'].item()\n",
    "\n",
    "    ax[2].plot(plot_time, pre_HF_EMG[CA_chan], color = 'b', label = 'Pre-HF')\n",
    "    ax[2].plot(plot_time, HF_EMG[CA_chan], color = 'r', label = 'During HF')\n",
    "    ax[2].plot(plot_time, post_HF_EMG[CA_chan], color = 'g', label = 'Post-HF')\n",
    "    \n",
    "    EMG1_std_data = np.std(pre_HF_EMG[CA_chan])\n",
    "    if offsets is not None:\n",
    "        EMG1_y_lim = [(-EMG1_std_data * auto_scale_factor) - EMG_dy[CA_chan], (EMG1_std_data * auto_scale_factor) + EMG_dy[CA_chan]]    \n",
    "    else:\n",
    "        EMG1_y_lim = [-EMG1_std_data * auto_scale_factor, EMG1_std_data * auto_scale_factor] \n",
    "\n",
    "    ax[2].legend(loc='upper right')\n",
    "    ax[2].set(ylabel = 'Voltage (uV)', title='Cricoarytenoid EMG', xlim = (0,20), ylim=(EMG1_y_lim[0],EMG1_y_lim[1]))\n",
    "\n",
    "    \"\"\"CT EMG\"\"\"\n",
    "    CT_chan = metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['CT EMG Channel'].item()\n",
    "\n",
    "    ax[3].plot(plot_time, pre_HF_EMG[CT_chan], color = 'b', label = 'Pre-HF')\n",
    "    ax[3].plot(plot_time, HF_EMG[CT_chan], color = 'r', label = 'During HF')\n",
    "    ax[3].plot(plot_time, post_HF_EMG[CT_chan], color = 'g', label = 'Post-HF')\n",
    "\n",
    "    EMG2_std_data = np.std(pre_HF_EMG[CT_chan])\n",
    "    if offsets is not None:\n",
    "        EMG2_y_lim = [(-EMG2_std_data * auto_scale_factor) - EMG_dy[CT_chan], (EMG2_std_data * auto_scale_factor) + EMG_dy[CT_chan]]    \n",
    "    else:\n",
    "        EMG2_y_lim = [-EMG2_std_data * auto_scale_factor, EMG2_std_data * auto_scale_factor] \n",
    "\n",
    "    ax[3].legend(loc='upper right')\n",
    "    ax[3].set(xlabel = 'Time (ms)', title='Cricothyroid EMG', xlim = (0,20), ylim=(EMG2_y_lim[0],EMG2_y_lim[1]))\n",
    "\n",
    "    \"\"\" Cardiac \"\"\"\n",
    "    physDF_idx = physDF.loc[ physDF['Stim Parameter'] == param].index\n",
    "    HR_plot_start = physDF.loc[physDF_idx, 'Start Index'].item() - 30001\n",
    "    HR_plot_stop = physDF.loc[physDF_idx, 'Stop Index'].item() + 30001\n",
    "    HR_plot_time = np.arange(0, (HR_plot_stop - HR_plot_start) / 1000, .001)\n",
    "\n",
    "    #For adding shading to plot\n",
    "    pulse_start = 30001\n",
    "    pulse_duration = physDF.loc[physDF_idx, 'Stop Index'].item() - physDF.loc[physDF_idx, 'Start Index'].item()\n",
    "    pulse_stop = pulse_start + pulse_duration\n",
    "    pulse_window = (pulse_start / 1000, pulse_stop / 1000)\n",
    "\n",
    "    phys_ch_slice = phys_data._ch_to_index(phys_channel)\n",
    "\n",
    "    ax[4].plot(HR_plot_time, np.squeeze(phys_data.array[phys_ch_slice, HR_plot_start:HR_plot_stop]))\n",
    "    ax[4].set( ylabel = 'HR (bpm)', title='Cardiac')\n",
    "\n",
    "    #Tonic Stim window\n",
    "    ax[4].axvspan(pulse_window[0], pulse_window[1], color = 'b', alpha = 0.1, label='Tonic Stim')\n",
    "\n",
    "    #HF window\n",
    "    HF_start = 30 + metaDF.loc[ metaDF['TDT Tank'] == tank_ID, 'HF Delay (s)'].item()  #30 is from the 30 second pre-tonic stim plot window\n",
    "    HF_stop = HF_start + HF_duration\n",
    "\n",
    "    ax[4].axvspan(HF_start, HF_stop, color = 'r', alpha = 0.3, label='HF')\n",
    "    ax[4].legend(loc='upper right')\n",
    "\n",
    "    fig_title = metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['Type'].item() + ': '\\\n",
    "                + str(metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['HF Amplitude (mA)'].item()) + ' mA HF || '\\\n",
    "                + str(abs(stimDF.loc[param, 'pulse amplitude (μA)']) / 1000) + ' mA LF ' + 'Ch. ' + str(stimDF.loc[param, 'channel'])\\\n",
    "                + ' || ' + str(param)\n",
    "\n",
    "    save_title = metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['Type'].item() + '_'\\\n",
    "               + str(metaDF.loc[ metaDF['TDT Tank'] == tank_ID]['HF Amplitude (mA)'].item()) + '_ma_HF_Ch'\\\n",
    "                + str(stimDF.loc[param, 'channel'])+'_StimAmp_'+ str(int(abs(stimDF.loc[param, 'pulse amplitude (μA)'])))+'_' + str(param)+'.png'\n",
    "    print(save_title)\n",
    "    fig.suptitle(fig_title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    if save_fig == True:\n",
    "        fig.savefig(save_title, format='png')\n",
    "        fig.clear()\n",
    "        plt.clf()\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def ADI_align(phys_data, ADI_filePATH, metaDF, stim):\n",
    "    \"\"\" Physiology Data Import\"\"\"\n",
    "    comMATRIX = sio.loadmat(ADI_filePATH)\n",
    "    \n",
    "    \"\"\"Import comments into new more accessible DF and align them with sample #'s of the entire file\"\"\"\n",
    "    comDF = pd.DataFrame()\n",
    "    comDF[['Comment Text' ,'Sample #', 'Stim Train ID']] = \"\"\n",
    "    comDF['Comment Matrix ID'] = comMATRIX['com'][:,4]\n",
    "    \n",
    "    for i,V in enumerate(comDF['Comment Matrix ID']): \n",
    "        comDF.loc[i, 'Comment Text'] = comMATRIX['comtext'][int(V-1)].strip()\n",
    "    \n",
    "    #Calculates the # samples per ADI block\n",
    "    blockLENGTHS = comMATRIX['dataend'] - comMATRIX['datastart']\n",
    "    blockIDX = []\n",
    "    \n",
    "    #Constructs a list of tuples containing the start and stop indices of each ADI block.\n",
    "    #This is necessary to find the exact sample # of each comment due to how they are mapped by ADI i.e. the 'com' array gives the index of the comment within the block it was made -- not for the whole file\n",
    "    for i,V in enumerate(blockLENGTHS[0,:]):\n",
    "        if i == 0:\n",
    "            blockIDX.append((0,V))\n",
    "            off = V\n",
    "        else:\n",
    "            on = off + 1\n",
    "            off = off + V\n",
    "            blockIDX.append((on,off))\n",
    "    \n",
    "    #Iterate through comments matrix to find the index values of the comments and assigns them to the comment list\n",
    "    for i,V in enumerate(comMATRIX['com']):\n",
    "        blockNUM = int(V[1] - 1) #V[1] is the block that # that the comment was made in.  Subtract 1 to get it into python indexing (starts @ 0)    \n",
    "        comDF.loc[i, 'Sample #'] = blockIDX[blockNUM][0] + V[2] #blockIDX[blockNUM][0] = Starting sample # of the block the comment is in.  V[2] is the sample # within that block that the comment was made.\n",
    "    \n",
    "    \"\"\" Code segment which looks for on/offs in the stim output from the TDT to ADI.  If the ADI data is noisy or messed up this may need tweaking.  Generally, there should be an equal number of start / stop indices assuming the system started with the stim trigger output on 0V.\"\"\"\n",
    "    phys_ch_slice = phys_data._ch_to_index('Stim Trigger')\n",
    "    dataDF = pd.DataFrame(data = np.squeeze(phys_data.array[phys_ch_slice].compute()), columns=['Stim Trigger'])\n",
    "    \n",
    "    start_idx = dataDF.loc[(dataDF['Stim Trigger'] > 2.5) & (dataDF['Stim Trigger'].shift() <2.5) & (dataDF['Stim Trigger'].shift(10) < 0.5)].index\n",
    "    print('Found ' + str(len(start_idx)) + ' start indices')\n",
    "    stop_idx = dataDF.loc[(dataDF['Stim Trigger'] < 0.5) & (dataDF['Stim Trigger'].shift() > 0.5) & (dataDF['Stim Trigger'].shift(10) > 3)].index\n",
    "    #TODO: make this an if statement check?\n",
    "    #stop_idx = stop_idx[1:] #This line removes the first element of the stop indices that would occur if the system started with the stim trigger ouput at 3.3V\n",
    "    print('Found ' + str(len(stop_idx)) + ' stop indices')\n",
    "    \n",
    "    \"\"\"Creates a dataframe for mapping comments to stim pulse trains\"\"\"\n",
    "    mapDF = pd.DataFrame(columns = ['Comment Text', 'Start Index', 'Stop Index', 'Stim Parameter', 'Max HR Change'])\n",
    "    \n",
    "    \"\"\"Assigns start/stop indexes every pulse train (in order)\"\"\"\n",
    "    mapDF['Start Index'] = start_idx\n",
    "    mapDF['Stop Index'] = stop_idx\n",
    "    mapDF['#Samples/Stim Train'] = mapDF['Stop Index'] - mapDF['Start Index']\n",
    "    \n",
    "    \"\"\" Lines up the comments with the pulse train by finding the closest pulse train start sample # to the comment's sample # \"\"\"\n",
    "    for i,V in enumerate(comDF['Sample #']):\n",
    "       dist = (mapDF['Start Index'] - V).abs()\n",
    "       mapDF.loc[dist.idxmin(), 'Comment Text'] = comDF['Comment Text'][i]\n",
    "    \n",
    "    \"\"\"Up to this point everything has been done with just ADI data. The code below pulls in data from the stimulation\n",
    "    object of pyeCAP to match up the stim parameters with the ADI data.\"\"\"\n",
    "    \n",
    "    \"\"\"Finds the pulse train # which lines up with the specific comment corresponding to each TDT tank.\"\"\"\n",
    "    comment_to_pulse_indicies = []\n",
    "    for idx,com in enumerate(metaDF['ADI File Comment']):\n",
    "        try:\n",
    "            comment_to_pulse_indicies.append(mapDF.index[mapDF['Comment Text'] == com][0])\n",
    "        except IndexError:\n",
    "            traceback.print_exc()\n",
    "            print('Could not find a comment match for TDT Tank: ' + metaDF.loc[idx, 'TDT Tank'])\n",
    "            print('Erred Comment: ' + str(com))\n",
    "            break\n",
    "    #comment_to_pulse_indicies = [mapDF.index[mapDF['Comment Text'] == com][0] for com in metaDF['ADI File Comment']]\n",
    "    \n",
    "    \"\"\"Maps the individual pulse trains to their specific stim index parameters\"\"\"\n",
    "    stats_index_dict = {key:value for (key,value) in zip(metaDF['TDT Tank'].to_list(), comment_to_pulse_indicies)}\n",
    "    grouped_parameters = stim.parameters.groupby(level=0).groups\n",
    "    \n",
    "    for tank_idx, tank in enumerate(metaDF['TDT Tank'].to_list()):\n",
    "        for i in np.arange(grouped_parameters[tank_idx].size):\n",
    "            mapDF.at[stats_index_dict[tank] + i, 'Stim Parameter'] = grouped_parameters[tank_idx][i]\n",
    "    return mapDF\n",
    "\n",
    "def get_parameters(type=None, HF_amp = None, LF_amp = None, stim_ch = None):\n",
    "    paramLIST = []\n",
    "    #Only Type is given\n",
    "    if type is not None and HF_amp is None and LF_amp is None and stim_ch is None:\n",
    "        tanks = metaDF.loc[(metaDF['Type'] == type)]['TDT Tank'].to_list()\n",
    "        for tank in tanks:\n",
    "            paramLIST.extend(tank_to_params_dict[tank].to_list())\n",
    "    #Only HF_amp is given\n",
    "    elif type is None and HF_amp is not None and LF_amp is None and stim_ch is None:\n",
    "        tanks = metaDF.loc[ (metaDF['HF Amplitude (mA)'] == HF_amp)]['TDT Tank'].to_list()\n",
    "        for tank in tanks:\n",
    "            paramLIST.extend(tank_to_params_dict[tank].to_list())\n",
    "    #Only Type and HF amp are given\n",
    "    elif type is not None and HF_amp is not None and LF_amp is None and stim_ch is None:\n",
    "        tanks = metaDF.loc[ (metaDF['Type'] == type) & (metaDF['HF Amplitude (mA)'] == HF_amp)]['TDT Tank'].item()\n",
    "        paramLIST.extend(tank_to_params_dict[tanks])  \n",
    "    else:\n",
    "        tank = metaDF.loc[ (metaDF['Type'] == type) & (metaDF['HF Amplitude (mA)'] == HF_amp)]['TDT Tank'].item()\n",
    "        paramDF = stim.parameters.loc[tank_to_params_dict[tank]]\n",
    "        paramLIST = paramDF.loc[ (paramDF['pulse amplitude (μA)'] == LF_amp) & (paramDF['channel'] == stim_ch)].index.item()\n",
    "    return paramLIST\n",
    "\n",
    "def find_HF(param, ECAP_data, ECAP_channel, metaDF, tank_ID, plot = False):\n",
    "    data = ECAP_data.array(param, ECAP_channel)[param]\n",
    "    pulse_std = np.std(data, axis=2)\n",
    "    \n",
    "    HF_delay = metaDF.loc[ metaDF['TDT Tank'] == tank_ID, 'HF Delay (s)'].item()\n",
    "    HF_duration = metaDF.loc[ metaDF['TDT Tank'] == tank_ID, 'HF Duration (s)'].item()\n",
    "    \n",
    "    pulses_per_sec = ECAP_data.stim.parameters.at[param, 'frequency (Hz)']\n",
    "    baseline_pulse_stop = int((HF_delay - 1) * pulses_per_sec)\n",
    "    baseline = np.mean(pulse_std[ 0 : baseline_pulse_stop])\n",
    "    \n",
    "    HF_window_start = int( (HF_delay + 2) * pulses_per_sec)\n",
    "    HF_window_stop = int(( HF_delay + HF_duration - 2) * pulses_per_sec)\n",
    "    HF = np.mean(pulse_std[ HF_window_start : HF_window_stop ])\n",
    "    HF_threshold = HF - ((HF - baseline) * 0.5)\n",
    "    \n",
    "    HF_indices = np.nonzero(pulse_std > HF_threshold)[0]\n",
    "    HF_onset_pulse = HF_indices[0]\n",
    "    HF_offset_pulse = HF_indices[-1]\n",
    "    \n",
    "    if plot:\n",
    "        plt.plot(np.arange(data.shape[0]), pulse_std)\n",
    "        #plt.axhline(baseline, color='c')\n",
    "        #plt.axhline(HF, color='g')\n",
    "        plt.axhline(HF_threshold, color='r')\n",
    "        plt.show()\n",
    "    \n",
    "    return [HF_onset_pulse, HF_offset_pulse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2207447c8b3aaf6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:57:15.617619Z",
     "start_time": "2024-09-24T19:57:15.586612Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Import meta data\"\n",
    "\"\"\"Get HF meta data from excel file\"\"\"\n",
    "#excel_path = r'E:\\HF_Block\\20240904_HFBlock_Pig05\\20240904_HFBlock_05_Parameters_modified.xlsx'\n",
    "excel_path = r'G:\\Data\\HF_Block\\HF_Block_Project_summary.xlsx'\n",
    "metaDF = pd.read_excel(excel_path, sheet_name = 'HF05_metaDF')\n",
    "\n",
    "\"\"\"Remove rows that are not needed for current analysis and reset index of metaDF\"\"\"\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'HF Only'].index, inplace = True)  #Removed HF only tanks as they did not have any stim parameters\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'Control'].index, inplace = True)\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'X'].index, inplace = True)\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'Thresholding'].index, inplace = True)\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'Contact ID'].index, inplace = True)\n",
    "metaDF.reset_index(inplace = True)\n",
    "#metaDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d93dd5e731e4c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:57:19.652811Z",
     "start_time": "2024-09-24T19:57:15.634622Z"
    }
   },
   "outputs": [],
   "source": [
    "\"Import meta data and TDT files\"\n",
    "\"\"\"Get HF meta data from excel file\"\"\"\n",
    "#excel_path = r'E:\\HF_Block\\20240904_HFBlock_Pig05\\20240904_HFBlock_05_Parameters_modified.xlsx'\n",
    "excel_path = r'G:\\Data\\HF_Block\\HF_Block_Project_summary.xlsx'\n",
    "metaDF = pd.read_excel(excel_path, sheet_name = 'HF05_metaDF')\n",
    "\n",
    "\"\"\"Remove rows that are not needed for current analysis and reset index of metaDF\"\"\"\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'HF Only'].index, inplace = True)  #Removed HF only tanks as they did not have any stim parameters\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'Control'].index, inplace = True)\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'X'].index, inplace = True)\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'Thresholding'].index, inplace = True)\n",
    "metaDF.drop(metaDF.loc[ metaDF['Type'] == 'Contact ID'].index, inplace = True)\n",
    "metaDF.reset_index(inplace = True)\n",
    "\n",
    "\"TDT Data Import - List\"\n",
    "#tdt_path = r'E:\\HF_Block\\20240904_HFBlock_Pig05\\HF_Block_Template-240904-083702' #Lab Path\n",
    "tdt_path = r'G:\\Data\\HF_Block\\20240904_HFBlock_Pig05\\HF_Block_Template-240904-083702' #Local Path \n",
    "tdt_file_list_all = os.listdir(tdt_path)\n",
    "tdt_file_list_all.remove('desktop.ini')\n",
    "\n",
    "#ADI_path = r'E:\\HF_Block\\20240904_HFBlock_Pig05' #Lab Path\n",
    "ADI_path = r'G:\\Data\\HF_Block\\20240904_HFBlock_Pig05' #Local path\n",
    "ADI_file = r'\\20240904_HFBlock_Pig05_stats_modified.mat'\n",
    "\n",
    "raw_phys = pyeCAP.Phys(ADI_path + ADI_file, mult_data=False)\n",
    "ws_stripped_names = [i.strip() for i in raw_phys.ch_names] #This line just removes the white spaces around the names imported\n",
    "raw_phys = raw_phys.set_ch_names(ws_stripped_names)\n",
    "\n",
    "\"\"\"Ephys data instantiation\"\"\"\n",
    "tdt_path_list = [tdt_path + '\\\\' + fileNAME for fileNAME in metaDF['TDT Tank']]\n",
    "\n",
    "#Data Streams\n",
    "#raw_ECAP = pyeCAP.Ephys(tdt_path_list, stores = 'ECAP')\n",
    "#raw_ECAP = raw_ECAP.remove_ch(['ECAP 6', 'ECAP 7', 'ECAP 8'])\n",
    "\n",
    "\"\"\"Changes the stim multi-index to utilize the Tank names from tdt_file_list instead [0,1,2,3...]\"\"\"\n",
    "stim = pyeCAP.Stim(tdt_path_list)\n",
    "stimDF = stim.parameters\n",
    "#tank_to_params_dict = { tank:params for (tank,params) in zip(metaDF['TDT Tank'].to_list(), stim.parameters.groupby(level=0).groups.values() ) }\n",
    "#params_to_tank_dict = { param:tank for (param,tank) in zip(stim.parameters.groupby(level=0).groups.keys(), metaDF['TDT Tank'].to_list()) }\n",
    "\n",
    "\"\"\"ECAP/EMG/TriCAP object instantiation\"\"\"\n",
    "#ECAPr = pyeCAP.ECAP(raw_ECAP, stim)\n",
    "PHYS = pyeCAP.ECAP(raw_phys,stim)\n",
    "raw_ECG_array = np.squeeze(raw_phys.array[raw_phys._ch_to_index('EKG Raw')].compute() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f5068be410b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Example code for finding parameters to plot\"\"\"\n",
    "tanks = metaDF.loc[ (metaDF['Type'] == 'Cardiac DRC') & (metaDF['HF Amplitude (mA)'] == 6)]['TDT Tank']\n",
    "for tank in tanks:\n",
    "    print(tank)\n",
    "for param in tank_to_params_dict[tank]:\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc267eee3ddceb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = get_parameters(type='EMG DRC', HF_amp = 4)\n",
    "print(parameters)\n",
    "\n",
    "for param in parameters:\n",
    "    pentaPLOT(param, ECAP_channel = 0, params_to_tank_dict=params_to_tank_dict, metaDF=metaDF, physDF=physDF, ECAP_data = ECAPf, EMG_data = EMGf, TriCAP_data = triCAPf, phys_data =  phys_data, phys_channel = 'HR Pulse Smooth', auto_scale_factor = 15, offsets = 6, save_fig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1403d9ce9d0860e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = stimDF.index\n",
    "for param in parameters:\n",
    "    pentaPLOT(param, ECAP_channel = 0, params_to_tank_dict=params_to_tank_dict, metaDF=metaDF, physDF=physDF, ECAP_data = ECAPf, EMG_data = EMGf, TriCAP_data = triCAPf, phys_data =  phys_data, phys_channel = 'HR Pulse Smooth', auto_scale_factor = 15, offsets = 6, save_fig = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80da13306ed078",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_ECG_array = np.squeeze( phys_data.array[phys_data._ch_to_index('EKG Raw')].compute() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a8fe40a0c2630",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = phys_data.pan_tompkins_algorithm(ecg_ch_name = 'EKG Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706d89335e8f0aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:36:35.739393Z",
     "start_time": "2024-09-24T17:36:34.788553Z"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Band-pass Filter\"\"\"\n",
    "ECG_fs = 1000\n",
    "nyq_rate = ECG_fs / 2\n",
    "\n",
    "cutoffs = [5,15]\n",
    "sos = signal.butter(4, cutoffs, btype='bandpass', fs = ECG_fs,output='sos')\n",
    "\n",
    "\"\"\"Code which replaces any NaN values by simply replacing them with the first value after the NaN's occur\"\"\"\n",
    "nan_idx = np.argwhere(np.isnan(raw_ECG_array))\n",
    "print(nan_idx)\n",
    "#np.nan_to_num(raw_ECG_array, copy=False, nan=raw_ECG_array[nan_idx[-1] + 1])\n",
    "\n",
    "ECG_BP_filt = signal.sosfiltfilt(sos, raw_ECG_array)\n",
    "ECG_dvt_filt = np.square(np.diff(ECG_BP_filt,append=0))\n",
    "#ECG_sq = np.square(ECG_dvt_filt)\n",
    "#ECG_moving_average = uniform_filter1d(ECG_sq,size=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f66262ddfbcfbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:36:38.571589Z",
     "start_time": "2024-09-24T17:36:38.486556Z"
    }
   },
   "outputs": [],
   "source": [
    "filt_data, data_diff_sq, data_moving_average = phys_data.pan_tompkins_algorithm(ecg_ch_name = 'EKG Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8660a3ff26f72",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:37:07.525414Z",
     "start_time": "2024-09-24T17:37:07.118690Z"
    }
   },
   "outputs": [],
   "source": [
    "filt_data_array = np.squeeze(da.concatenate(filt_data,axis=1).compute())\n",
    "#diff_sq_array = np.squeeze(da.concatenate(data_diff_sq,axis=0).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d685f4ad6f596",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:12:55.291693Z",
     "start_time": "2024-09-24T17:12:54.412286Z"
    }
   },
   "outputs": [],
   "source": [
    "comMATRIX = sio.loadmat(ADI_path + ADI_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b21e23cf869e97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:12:55.385226Z",
     "start_time": "2024-09-24T17:12:55.370222Z"
    }
   },
   "outputs": [],
   "source": [
    "data = comMATRIX['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb31774a73b066",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:13:01.488044Z",
     "start_time": "2024-09-24T17:13:01.468039Z"
    }
   },
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233e89661fa32034",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:13:09.151548Z",
     "start_time": "2024-09-24T17:13:09.146547Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38f9bd63f55e670",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:13:22.404250Z",
     "start_time": "2024-09-24T17:13:22.392248Z"
    }
   },
   "outputs": [],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49bf4b2e61b89b1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:16:22.285397Z",
     "start_time": "2024-09-24T17:16:22.273395Z"
    }
   },
   "outputs": [],
   "source": [
    "comMATRIX['datastart'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed3a48059040f9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:27:30.364495Z",
     "start_time": "2024-09-24T17:27:30.359494Z"
    }
   },
   "outputs": [],
   "source": [
    "indices = [[start,stop] for start,stop in zip(comMATRIX['datastart'][5].astype(int), comMATRIX['dataend'][5].astype(int))]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b4ea3a04a5cbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:27:31.925191Z",
     "start_time": "2024-09-24T17:27:31.881484Z"
    }
   },
   "outputs": [],
   "source": [
    "dataLIST = []\n",
    "for idx in indices:\n",
    "    dataLIST.append(comMATRIX['data'][0,idx[0]:idx[1]])\n",
    "data = np.concatenate(dataLIST, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12666b476db0ca6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:27:32.507588Z",
     "start_time": "2024-09-24T17:27:32.489583Z"
    }
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c469341bd3789356",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T17:27:33.437426Z",
     "start_time": "2024-09-24T17:27:33.401418Z"
    }
   },
   "outputs": [],
   "source": [
    "nanindex = np.nonzero(np.isnan(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77919bcf58e7028",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:31:00.521633Z",
     "start_time": "2024-09-24T19:31:00.514630Z"
    }
   },
   "outputs": [],
   "source": [
    "phys_data.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1edffe24bc364",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:35:23.944021Z",
     "start_time": "2024-09-24T19:35:23.932016Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx,val in enumerate(raw_phys.data):\n",
    "    print(idx)\n",
    "    test = val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebf9163449d09e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:35:26.447755Z",
     "start_time": "2024-09-24T19:35:26.432753Z"
    }
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c839c4a58a0dbb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:55:14.423732Z",
     "start_time": "2024-09-24T19:55:11.893610Z"
    }
   },
   "outputs": [],
   "source": [
    "test.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7444f1c2fb38ba6c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:57:26.241622Z",
     "start_time": "2024-09-24T19:57:26.237620Z"
    }
   },
   "outputs": [],
   "source": [
    "ignore = raw_phys.remove_cautery(ecg_ch_name = 'EKG Raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913df4aea7669321",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:57:43.391571Z",
     "start_time": "2024-09-24T19:57:43.323558Z"
    }
   },
   "outputs": [],
   "source": [
    "ignore.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9733deb73a975970",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-24T19:58:02.733067Z",
     "start_time": "2024-09-24T19:58:02.718064Z"
    }
   },
   "outputs": [],
   "source": [
    "ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5284529f29f7cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
