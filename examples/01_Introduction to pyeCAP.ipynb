{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01 - Introduction to pyeCAP\n",
    "\n",
    "Python is a python package for the analysis of evoked compound action potentials (CAPs) in elecytrophysiology data sets. This program is currently under development and all classes and methods may be subject to change.\n",
    "\n",
    "pyeCAP was built with the goal of simplifying the collection and analysis of CAP data. As such this toolkit abstracts the technicalities of loading, saving, and working with ephys data into an efficient class structure that forms the basis for fast and interactive analysis of CAPs. This enables real-time visualization of data during an ephys recording session to allow for optimizing experimental time and resources. The pyeCAP package also contains many visualization tools for fast and interactive visualization and analysis of data after an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization of pyeCAP\n",
    "\n",
    "pyeCAP is organization into three major classes that you user will interact with. These are:\n",
    "* __Ephys__ - A class for working with Ephys data. This class handles mapping ephys data sets from disk for fast analysis, preprocessing, and visualization of data. Important features include:\n",
    " - _Loading Data_\n",
    " - _Interactive Visualization and Plotting_\n",
    " - _Re-referencing strategies_\n",
    " - _Filtering_\n",
    "* __Stim__ - A class for working with Stimulation data. This class handles the details of reading in stimulation data from different stimulation/recording system. Timing of stimulation pulse trains, individual pulses, and stimulation parameters can be easily worked with through the Stim class.\n",
    "* __ECAP__ - The ECAP class works with Ephys and Stim data and provides an interface for analysis and visualization of ECAP data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Data Types\n",
    "\n",
    "eba_toolkit currently supports analysis of data collected with systems developed by Tucker Davis Technologies (https://www.tdt.com/), ADInstruments (https://www.adinstruments.com/), and Ripple Neuro (https://rippleneuro.com/) [Coming Soon]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Interactive Data Analysis\n",
    "\n",
    "In order to enable interactive data analysis and both real-time data as well as those stored on your computers hard disk, pyeCAP is built on the concept of lazy execution. Electrophysiology analysis scripts written by scientists are often time consuming and memory intensive to run. This is because we are often working with large data sets and in order to preprocess our data we usually read the entire data set into memory and often create copies of the data after various filtering, referencing, artifact rejection, or other procedures. Additionally, although we normally preprocess an entire data set we often only work with and plot small portions of our data at a time. \n",
    "\n",
    "pyeCAP is built from the ground up to work differently. Data is mapped to it's location on your hard disk, without ever reading the data into memory. A data analysis pipeline, wich can consist of various preprocessing or data analysis techniques is then built by the user. At this point zero computation has been performed. \n",
    "\n",
    "To interact with your data, at some point you will want to reduce processed data into some kind of summary statistics or visualization. At this point pyeCAP evaluates your data analysis procedure and determines with data needs to be accessed to return the request result. In this way it only reads from disk and analyzes the section of data that are relevant to the reduced data set on which you are performing statistics or creating visualizations from.\n",
    "\n",
    "Chunking data in this way allows for minimizes unnecessary computations, but also allows us to easily parallelize computations across multiple chunks of data in order to take advantage of parallel processing and further accelerate your data analysis. Lastly, because chunks of data are not all stored in memory at once. pyeCAP allows you to work with large data sets on even modest computer hardware and even work on data sets that are larger than your available memory. \n",
    "\n",
    "For those interested. pyeCAP mainly takes advantage of a python project called Dask to parallelize operations on chunked data sets. More information on Dask and lazy evaluation can be found here: https://docs.dask.org/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a Data Set\n",
    "\n",
    "Let's get started by reading in and plotting a data set with pyECAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyeCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment this code to download and unzip tdt tanks from web hosting urls\n",
    "\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "# Download the example data into the current directory\n",
    "# This may take a while since the files are large\n",
    "urls = ['https://gin.g-node.org/Jtrevathan/pyeCAP/raw/master/data/pnpig191126-191204-174838.zip',\n",
    "        'https://gin.g-node.org/Jtrevathan/pyeCAP/raw/master/data/pnpig191126-191204-175046.zip']\n",
    "for url in urls:\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size_in_bytes= int(response.headers.get('content-length', 0))\n",
    "    block_size = 1024 #1 Kibibyte\n",
    "    progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "    with open(f'{url.split(\"/\")[-1]}', \"wb\") as f:\n",
    "        for block in response.iter_content(block_size):\n",
    "            progress_bar.update(len(block))\n",
    "            f.write(block)\n",
    "    progress_bar.close()\n",
    "    if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "        raise Exception(\"File download failed.\")\n",
    "        \n",
    "print(\"Files Downloaded! - Unzipping.\")\n",
    "\n",
    "for url in urls:\n",
    "    with zipfile.ZipFile(f'{url.split(\"/\")[-1]}', 'r') as z:\n",
    "        z.extractall(f'./')\n",
    "    os.remove(f'{url.split(\"/\")[-1]}')\n",
    "print(\"Files Unzipped Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a data set collected with a Tucker Davis Technologies (TDT) recording system. This is called a TDT tank and is stored as a directory containing a set of related files. \n",
    "\n",
    "Note that importing the data set is near instantaneous. This is because instantiating an instance of the Ephys class merely creates a mapping to the data stored on your hard disk and does not actually read the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory containing the TDT tank\n",
    "directory = r\"pnpig191126-191204-174838\"\n",
    "data = pyeCAP.Ephys(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can create a simple plot of the first 5 seconds of data on all of the channels in this data set.\n",
    "\n",
    "Once again plotting of the data should be quite fast since, since only the data necesary to make the requested plot will be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot(x_lim=(0, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
