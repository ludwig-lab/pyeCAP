{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 01 - Introduction to pyeCAP\n",
    "\n",
    "Python is a python package for the analysis of evoked compound action potentials (CAPs) in elecytrophysiology data sets. This program is currently under development and all classes and methods may be subject to change.\n",
    "\n",
    "pyeCAP was built with the goal of simplifying the collection and analysis of CAP data. As such this toolkit abstracts the technicalities of loading, saving, and working with ephys data into an efficient class structure that forms the basis for fast and interactive analysis of CAPs. This enables real-time visualization of data during an ephys recording session to allow for optimizing experimental time and resources. The pyeCAP package also contains many visualization tools for fast and interactive visualization and analysis of data after an experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Organization of pyeCAP\n",
    "\n",
    "pyeCAP is organization into three major classes that you user will interact with. These are:\n",
    "* __Ephys__ - A class for working with Ephys data. This class handles mapping ephys data sets from disk for fast analysis, preprocessing, and visualization of data. Important features include:\n",
    " - _Loading Data_\n",
    " - _Interactive Visualization and Plotting_\n",
    " - _Re-referencing strategies_\n",
    " - _Filtering_\n",
    "* __Stim__ - A class for working with Stimulation data. This class handles the details of reading in stimulation data from different stimulation/recording system. Timing of stimulation pulse trains, individual pulses, and stimulation parameters can be easily worked with through the Stim class.\n",
    "* __ECAP__ - The ECAP class works with Ephys and Stim data and provides an interface for analysis and visualization of ECAP data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supported Data Types\n",
    "\n",
    "pyeCAP currently supports analysis of data collected with systems developed by Tucker Davis Technologies (https://www.tdt.com/), ADInstruments (https://www.adinstruments.com/), and Ripple Neuro (https://rippleneuro.com/) [Coming Soon]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enabling Interactive Data Analysis\n",
    "\n",
    "In order to enable interactive data analysis and both real-time data as well as those stored on your computers hard disk, pyeCAP is built on the concept of lazy execution. Electrophysiology analysis scripts written by scientists are often time consuming and memory intensive to run. This is because we are often working with large data sets and in order to preprocess our data we usually read the entire data set into memory and often create copies of the data after various filtering, referencing, artifact rejection, or other procedures. Additionally, although we normally preprocess an entire data set we often only work with and plot small portions of our data at a time. \n",
    "\n",
    "pyeCAP is built from the ground up to work differently. Data is mapped to it's location on your hard disk, without ever reading the data into memory. A data analysis pipeline, wich can consist of various preprocessing or data analysis techniques is then built by the user. At this point zero computation has been performed. \n",
    "\n",
    "To interact with your data, at some point you will want to reduce processed data into some kind of summary statistics or visualization. At this point pyeCAP evaluates your data analysis procedure and determines with data needs to be accessed to return the request result. In this way it only reads from disk and analyzes the section of data that are relevant to the reduced data set on which you are performing statistics or creating visualizations from.\n",
    "\n",
    "Chunking data in this way allows for minimizes unnecessary computations, but also allows us to easily parallelize computations across multiple chunks of data in order to take advantage of parallel processing and further accelerate your data analysis. Lastly, because chunks of data are not all stored in memory at once. pyeCAP allows you to work with large data sets on even modest computer hardware and even work on data sets that are larger than your available memory. \n",
    "\n",
    "For those interested. pyeCAP mainly takes advantage of a python project called Dask to parallelize operations on chunked data sets. More information on Dask and lazy evaluation can be found here: https://docs.dask.org/en/latest/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a Data Set\n",
    "\n",
    "Let's get started by reading in and plotting a data set with pyECAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pyeCAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import hashlib # hashlib for generating hashes of files\n",
    "import os # os for interacting with the operating system\n",
    "import requests # requests for making HTTP requests\n",
    "import zipfile # zipfile for extracting zip files\n",
    "from tqdm import tqdm # tqdm for progress bars\n",
    "\n",
    "# URLs of the data files to be downloaded\n",
    "urls = [\n",
    "    'https://gin.g-node.org/Jtrevathan/pyeCAP/raw/master/data/pnpig191126-191204-174838.zip',\n",
    "    'https://gin.g-node.org/Jtrevathan/pyeCAP/raw/master/data/pnpig191126-191204-175046.zip'\n",
    "]\n",
    "\n",
    "# Expected hashes for the files\n",
    "# These are used to verify the integrity of the downloaded files\n",
    "expected_hashes = {\n",
    "    'pnpig191126-191204-174838': '2c3e6c022dae968bc35d4e01c595c004',\n",
    "    'pnpig191126-191204-175046': '6d638c7fcf00ee9fff3d39d9cb5e39b3'\n",
    "}\n",
    "\n",
    "# Function to calculate the hash of a folder\n",
    "# This is used to verify the integrity of the downloaded files\n",
    "def calculate_folder_hash(folder_path, hash_type=\"md5\"):\n",
    "    # Create a new hash object\n",
    "    hash_obj = hashlib.md5()\n",
    "\n",
    "    # Walk through each file in the folder\n",
    "    for dirpath, dirnames, filenames in os.walk(folder_path):\n",
    "        for filename in filenames:\n",
    "            # Get the full path of the file\n",
    "            file_path = os.path.join(dirpath, filename)\n",
    "\n",
    "            # Update the hash object with the file's content\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                    hash_obj.update(chunk)\n",
    "\n",
    "    # Return the hexadecimal representation of the hash\n",
    "    return hash_obj.hexdigest()\n",
    "\n",
    "# Loop through each URL to download the data\n",
    "for url in urls:\n",
    "    # Extract the file name and folder name from the URL\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    folder_name = file_name.split(\".\")[0]\n",
    "\n",
    "    # Download if the folder does not exist or the hash is wrong\n",
    "    if not (os.path.exists(folder_name) and calculate_folder_hash(folder_name) == expected_hashes[folder_name]):\n",
    "        # Download the file\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size_in_bytes = int(response.headers.get('content-length', 0))\n",
    "        block_size = 1024  # 1 Kibibyte\n",
    "        # Create a progress bar for the download\n",
    "        progress_bar = tqdm(total=total_size_in_bytes, unit='iB', unit_scale=True)\n",
    "        # Write the content of the response to a file\n",
    "        with open(file_name, \"wb\") as f:\n",
    "            for block in response.iter_content(block_size):\n",
    "                progress_bar.update(len(block))\n",
    "                f.write(block)\n",
    "        progress_bar.close()\n",
    "        # Raise an exception if the download was not successful\n",
    "        if total_size_in_bytes != 0 and progress_bar.n != total_size_in_bytes:\n",
    "            raise Exception(\"File download failed.\")\n",
    "\n",
    "        print(\"File Downloaded! - Unzipping.\")\n",
    "\n",
    "        # Unzip the file and delete the zip file\n",
    "        with zipfile.ZipFile(file_name, 'r') as z:\n",
    "            z.extractall('./')\n",
    "        os.remove(file_name)\n",
    "        print(\"Files Unzipped Successfully!\")\n",
    "    else:\n",
    "        print(\"Files Already Downloaded\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import a data set collected with a Tucker Davis Technologies (TDT) recording system. This is called a TDT tank and is stored as a directory containing a set of related files. \n",
    "\n",
    "Note that importing the data set is near instantaneous. This is because instantiating an instance of the Ephys class merely creates a mapping to the data stored on your hard disk and does not actually read the data into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to directory containing the TDT tank\n",
    "directory = r\"pnpig191126-191204-174838\"\n",
    "\n",
    "# Create an instance of the Ephys class, which maps the data stored on the hard disk\n",
    "# Note that this does not actually read the data into memory\n",
    "data = pyeCAP.Ephys(directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can create a simple plot of the first 5 seconds of data on all of the channels in this data set.\n",
    "\n",
    "Once again plotting of the data should be quite fast since, since only the data necesary to make the requested plot will be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 5 seconds of data on all channels\n",
    "# Only the data necessary to make the plot will be accessed\n",
    "data.plot(x_lim=(0, 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
